{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e113661e",
      "metadata": {
        "copyright": "Материал разработан командой ThetaHat.",
        "id": "e113661e"
      },
      "source": [
        "# Машинное обучение, ШАД\n",
        "## Домашнее задание 11: Оптимизация нейросетей\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e03bf2c0",
      "metadata": {
        "copyright": "Материал разработан командой ThetaHat.",
        "id": "e03bf2c0"
      },
      "outputs": [],
      "source": [
        "from typing import Callable, Iterable, List, Optional, Tuple\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim\n",
        "from torch.optim import Optimizer\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "...  # допиши необходимые импорты\n",
        "\n",
        "sns.set(palette=\"Set2\")\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e7ad051",
      "metadata": {
        "copyright": "Материал разработан командой ThetaHat.",
        "id": "6e7ad051"
      },
      "source": [
        "### Ссылки на использование ИИ\n",
        "\n",
        "Если при решении задач использовался ИИ, укажи здесь публичные ссылки на все чаты с ИИ и поясни, для каких целей он применялся. Обрати внимание на <a href=\"https://thetahat.ru/courses/ai-rules\">правила</a>.\n",
        "\n",
        "**Задача 1**\n",
        "1. ссылка\n",
        "    - для чего использована\n",
        "    - для чего использована\n",
        "2. ссылка\n",
        "    - для чего использована\n",
        "\n",
        "**Задача 2**\n",
        "1. ссылка\n",
        "    - для чего использована\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kfU5PPIcUrc6",
      "metadata": {
        "copyright": "Материал разработан командой ThetaHat.",
        "id": "kfU5PPIcUrc6"
      },
      "source": [
        "---\n",
        "### Задача 1.\n",
        "\n",
        "Контест"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad5b0bb9",
      "metadata": {
        "copyright": "Материал разработан командой ThetaHat.",
        "id": "ad5b0bb9"
      },
      "source": [
        "---\n",
        "### Задача 2.\n",
        "\n",
        "Рассмотрим полносвязную нейронную сеть из трех слоев, которая принимает на вход $x \\in \\mathbb{R}^{d_0}$ и возвращает $y \\in \\mathbb{R}^{d_3}$:\n",
        "\n",
        "$$y_\\theta(x) = \\sigma_2 \\big( \\sigma_1 \\left( x^{\\top} W_1 + b_1 \\right)^{\\top} W_2 + b_2 \\big)^{\\top} W_3 + b_3,$$\n",
        "\n",
        "где\n",
        "\n",
        "* $W_1 \\in \\mathbb{R}^{d_0 \\times d_1}, b_1 \\in \\mathbb{R}^{d_1}$ &mdash; параметры 1-го слоя,\n",
        "\n",
        "* $W_2 \\in \\mathbb{R}^{d_1 \\times d_2}, b_2 \\in \\mathbb{R}^{d_2}$ &mdash; параметры 2-го слоя,\n",
        "\n",
        "* $W_3 \\in \\mathbb{R}^{d_2 \\times d_3}, b_3 \\in \\mathbb{R}^{d_3}$ &mdash; параметры 3-го слоя,\n",
        "\n",
        "* $\\theta = (W_1, b_1, W_2, b_2, W_3, b_3)$ — все параметры нейросети,\n",
        "\n",
        "* $\\sigma_1(x), \\sigma_1(x)$ &mdash; некоторые функции активации.\n",
        "\n",
        "  \n",
        "\n",
        "Пусть все веса инциализированы константой $c_1$, а все смещения &mdash; константой $c_2$. Покажите, что в процессе обучения данной нейросети ее параметры будут меняться следующим образом.\n",
        "* Веса $W_1$ могут поменяться, и не обязательно будут одинаковыми; аналогично с весами $W_3$ и смещением $b_3$.\n",
        "* Веса $W_2$ также могут поменять свое значение, но в каждый момент времени это значение будет одинаковым для всех весов $W_2$; аналогично со смещениями $b_1$ и $b_2$.\n",
        "\n",
        "Обобщите это утверждение на случай большего количества слоев в нейронной сети."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afb564ff",
      "metadata": {
        "copyright": "Материал разработан командой ThetaHat.",
        "id": "afb564ff"
      },
      "source": [
        "*Это теоретическая задача. Решение в виде кода будет оцениваться 0 баллов*."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad020087",
      "metadata": {
        "copyright": "Материал разработан командой ThetaHat.",
        "id": "ad020087"
      },
      "source": [
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f0647b5",
      "metadata": {
        "copyright": "Материал разработан командой ThetaHat.",
        "id": "0f0647b5"
      },
      "source": [
        "---\n",
        "### Задача 3."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b5746c7",
      "metadata": {
        "copyright": "Материал разработан командой ThetaHat.",
        "id": "7b5746c7"
      },
      "source": [
        "**1.** В данной задаче вам предстоит реализовать метод оптимизации **Adam** (Adaptive Moment Estimation) путем создания собственного класса-оптимизатора в PyTorch. Предполагается следующая последовательность действий.\n",
        "\n",
        "\n",
        "1. Разработайте класс `MyAdam`, наследуемый от `torch.optim.Optimizer`\n",
        "\n",
        "2. Переопределите метод `step()`, содержащий логику метода `Adam`\n",
        "\n",
        "3. Реализуйте стандартные параметры метода:\n",
        "\n",
        "    - `lr` — скорость обучения (learning rate);\n",
        "\n",
        "    - `betas` — коэффициенты для вычисления скользящих средних моментов;\n",
        "\n",
        "    - `eps` — малая константа для обеспечения численной стабильности;\n",
        "\n",
        "    - `weight_decay` — коэффициент L2-регуляризации."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "285388f9",
      "metadata": {
        "copyright": "Материал разработан командой ThetaHat.",
        "id": "285388f9"
      },
      "outputs": [],
      "source": [
        "class My_Adam(Optimizer):\n",
        "    \"\"\"Класс, реализующий алгоритм Adam (Adaptive Moment Estimation).\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        params: Iterable[torch.Tensor],\n",
        "        lr: float = 1e-3,\n",
        "        betas: Tuple[float, float] = (0.9, 0.999),\n",
        "        eps: float = 1e-8,\n",
        "        weight_decay: float = 0,\n",
        "    ) -> None:\n",
        "        \"\"\"Инициализирует оптимизатор.\n",
        "\n",
        "        Параметры:\n",
        "            params: Итерируемый объект параметров для оптимизации\n",
        "            lr (float): Learning rate (скорость обучения)\n",
        "            betas (tuple): Коэффициенты для вычисления скользящих средних\n",
        "            eps (float): Термин для численной стабильности\n",
        "\n",
        "        \"\"\"\n",
        "        # Проверяем, что параметры корректны\n",
        "        if lr <= 0:\n",
        "            raise ValueError(f\"Learning rate должен быть положительным: {lr}\")\n",
        "        if not 0 <= betas[0] < 1:\n",
        "            raise ValueError(f\"beta должен быть между 0 и 1: {betas[0] }\")\n",
        "        if not 0 <= betas[1] < 1:\n",
        "            raise ValueError(f\"mu должен быть между 0 и 1: {betas[1]}\")\n",
        "        if eps <= 0:\n",
        "            raise ValueError(f\"eps должен быть положительным: {eps}\")\n",
        "\n",
        "        # Установить параметры по умолчанию\n",
        "        defaults = dict(...)\n",
        "        super(My_Adam, self).__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure: Optional[Callable[[], float]] = None) -> Optional[float]:\n",
        "        \"\"\"Выполняет один шаг оптимизации.\n",
        "\n",
        "        Параметры:\n",
        "            closure (callable, optional): Функция для перерасчета loss\n",
        "\n",
        "        Возвращает:\n",
        "            float: Значение loss если closure предоставлена, иначе None\n",
        "        \"\"\"\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        if closure is not None:\n",
        "            # Если передана closure функция, вычисляем loss\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "\n",
        "        # Пройти по всем группам параметров\n",
        "        for group in self.param_groups:\n",
        "            # Извлечь параметры группы\n",
        "            lr = group[\"lr\"]\n",
        "            beta, mu = group[\"betas\"]\n",
        "            eps = group[\"eps\"]\n",
        "            weight_decay = group[\"weight_decay\"]\n",
        "\n",
        "            # Пройти по всем параметрам в группе\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                ...\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9d5c84f",
      "metadata": {
        "copyright": "Материал разработан командой ThetaHat.",
        "id": "e9d5c84f"
      },
      "source": [
        "\n",
        "По аналогии с реализацией класса `Adam` реализуйте метод оптимизации `NAdam` — модификацию Adam с ускорением Нестерова. Для этого добавьте параметр `momentum_decay`, который контролирует влияние момента Нестерова."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56b8d935",
      "metadata": {
        "copyright": "Материал разработан командой ThetaHat.",
        "id": "56b8d935"
      },
      "outputs": [],
      "source": [
        "class My_NAdam(Optimizer):\n",
        "    \"\"\"Класс, реализующий алгоритм NAdam.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        params: Iterable[torch.Tensor],\n",
        "        lr: float = 1e-3,\n",
        "        betas: Tuple[float, float] = (0.9, 0.999),\n",
        "        eps: float = 1e-8,\n",
        "        weight_decay: float = 0,\n",
        "        momentum_decay: float = 0.004,\n",
        "    ) -> None:\n",
        "        \"\"\"Инициализирует оптимизатор.\n",
        "\n",
        "        Параметры:\n",
        "            params: Итерируемый объект параметров для оптимизации\n",
        "            lr (float): Learning rate (скорость обучения)\n",
        "            betas (tuple): Коэффициенты для вычисления скользящих средних\n",
        "            eps (float): Термин для численной стабильности\n",
        "\n",
        "        \"\"\"\n",
        "        # Проверяем, что параметры корректны\n",
        "        if lr <= 0:\n",
        "            raise ValueError(f\"Learning rate должен быть положительным: {lr}\")\n",
        "        if not 0 <= betas[0] < 1:\n",
        "            raise ValueError(f\"beta должен быть между 0 и 1: {betas[0]}\")\n",
        "        if not 0 <= betas[1] < 1:\n",
        "            raise ValueError(f\"mu должен быть между 0 и 1: {betas[1]}\")\n",
        "        if eps <= 0:\n",
        "            raise ValueError(f\"eps должен быть положительным: {eps}\")\n",
        "        if momentum_decay < 0:\n",
        "            raise ValueError(f\"momentum_decay должен быть неотрицательным: {momentum_decay}\")\n",
        "\n",
        "        # Установить параметры по умолчанию\n",
        "        defaults = dict(...)\n",
        "        super(My_NAdam, self).__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure: Optional[Callable[[], float]] = None) -> Optional[float]:\n",
        "        \"\"\"Выполняет один шаг оптимизации.\n",
        "\n",
        "        Параметры:\n",
        "            closure (callable, optional): Функция для перерасчета loss\n",
        "\n",
        "        Возвращает:\n",
        "            float: Значение loss если closure предоставлена, иначе None\n",
        "        \"\"\"\n",
        "\n",
        "        ...\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06501ccd",
      "metadata": {
        "copyright": "Материал разработан командой ThetaHat.",
        "id": "06501ccd"
      },
      "source": [
        "**2.** Проведем серию экспериментов для оценки работоспособности нашего оптимизатора.\n",
        "Введем вспомогательную функцию.\n",
        "\n",
        "Функция `make_experiment` визуализирует процесс оптимизации, отображая:\n",
        "\n",
        "* линии уровня целевой функции в логарифмической шкале,\n",
        "\n",
        "* траекторию движения алгоритма от начальной до конечной точки,\n",
        "\n",
        "* стартовую (зеленая) и финальную (синяя) точки."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de4ac6b5",
      "metadata": {
        "copyright": "Материал разработан командой ThetaHat.",
        "id": "de4ac6b5"
      },
      "outputs": [],
      "source": [
        "def make_experiment(\n",
        "    func: Callable[[torch.Tensor], float],\n",
        "    trajectory: List[Tuple[float, float]],\n",
        "    graph_title: str,\n",
        "    xlim: Tuple[float, float] = (-7, 7),\n",
        "    ylim: Tuple[float, float] = (-7, 7),\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Функция, которая для заданной функции рисует её линии уровня,\n",
        "    а также траекторию сходимости метода оптимизации.\n",
        "\n",
        "    Параметры:\n",
        "    1) func: Callable[[torch.Tensor], float] - оптимизируемая функция.\n",
        "       Принимает 2D-тензор (координаты) и возвращает число (значение функции).\n",
        "    2) trajectory: List[Tuple[float, float]] - траектория метода оптимизации.\n",
        "       Список кортежей, где каждый кортеж - это (x, y) точка.\n",
        "    3) graph_title: str - заголовок графика.\n",
        "    4) xlim: Tuple[float, float] - кортеж с границами оси X (min_x, max_x).\n",
        "    5) ylim: Tuple[float, float] - кортеж с границами оси Y (min_y, max_y).\n",
        "    \"\"\"\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "    # Создаем сетку для линий уровня\n",
        "    mesh_x = np.linspace(xlim[0], xlim[1], 300)\n",
        "    mesh_y = np.linspace(ylim[0], ylim[1], 300)\n",
        "    X, Y = np.meshgrid(mesh_x, mesh_y)\n",
        "    Z = np.zeros((len(mesh_x), len(mesh_y)))\n",
        "\n",
        "    # Вычисляем значения функции на сетке\n",
        "    for coord_x in range(len(mesh_x)):\n",
        "        for coord_y in range(len(mesh_y)):\n",
        "            Z[coord_y, coord_x] = func(torch.tensor((mesh_x[coord_x], mesh_y[coord_y])))\n",
        "\n",
        "    # Рисуем линии уровня\n",
        "    quantile_level = np.linspace((1e-4) ** (1.0 / 3), 0.95 ** (1.0 / 3), 20) ** 3\n",
        "    contour = np.unique(np.quantile(np.ravel(Z), quantile_level))\n",
        "    ax.contour(X, Y, np.log(Z), np.log(contour), cmap=\"winter\", linewidths=1.5, alpha=0.6, zorder=1)\n",
        "\n",
        "    # Рисуем всю траекторию целиком\n",
        "    xdata = [point[0] for point in trajectory]\n",
        "    ydata = [point[1] for point in trajectory]\n",
        "    ax.plot(xdata, ydata, \"ro-\", markersize=4, linewidth=1, zorder=2)\n",
        "\n",
        "    # Отмечаем начальную точку\n",
        "    ax.plot(xdata[0], ydata[0], \"go\", markersize=8, label=\"Старт\")\n",
        "\n",
        "    # Отмечаем конечную точку\n",
        "    ax.plot(xdata[-1], ydata[-1], \"bo\", markersize=8, label=\"Финиш\")\n",
        "\n",
        "    ax.set_title(graph_title)\n",
        "    ax.legend()\n",
        "    ax.set_xlim(xlim[0], xlim[1])\n",
        "    ax.set_ylim(ylim[0], ylim[1])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a1b659e",
      "metadata": {
        "copyright": "Материал разработан командой ThetaHat.",
        "id": "8a1b659e"
      },
      "source": [
        "Проведите серию экспериментов по оптимизации. Для этого сравните свои классы `Adam` и `NAdam` с реализациями `Adam` и `NAdam` из `torch`. Для каждого метода попробуйте минимум 2 набора гиперпараметров.\n",
        "\n",
        "Рассмотрите следующие функции\n",
        "\n",
        " $$\n",
        " f_1(x, y) = x^2 + y^2 + 10\\sin 5x + 10\\cos 5y\n",
        " $$\n",
        "\n",
        " $$\n",
        " f_2(x, y) = (1 - x)^2 + 100 \\left(y - x^2 \\right)^2\n",
        " $$\n",
        "\n",
        " $$\n",
        " f_3(x, y) = \\left(x^2 + y - 11\\right)^2 + \\left(x + y^2 - 7\\right)^2\n",
        " $$\n",
        "\n",
        "\n",
        "Для каждой связки (*функция, оптимизатор, стартовая точка*) выполните следующие действия.\n",
        "\n",
        "1. Создайте тензор `params` со стартовой точкой.\n",
        "\n",
        "2. Создайте экземпляр оптимизатора, передав ему этот тензор с параметрами.\n",
        "\n",
        "3. Создайте пустой список `trajectory` для сохранения истории точек $(x, y)$.\n",
        "\n",
        "4. Проведите цикл оптимизации.\n",
        "\n",
        "5. Для каждого эксперимента передайте полученный список `trajectory` в функцию `make_experiment`. Это позволит увидеть \"путь\" алгоритма по ландшафту функции.\n",
        "\n",
        "6. Сравните поведение оптимизаторов на разных функциях."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef38909b",
      "metadata": {
        "copyright": "Материал разработан командой ThetaHat.",
        "id": "ef38909b"
      },
      "outputs": [],
      "source": [
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68e4e89e",
      "metadata": {
        "copyright": "Материал разработан командой ThetaHat.",
        "id": "68e4e89e"
      },
      "source": [
        "---\n",
        "© 2025 команда <a href=\"https://thetahat.ru/\">ThetaHat</a> для курса ML-1 ШАД"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
