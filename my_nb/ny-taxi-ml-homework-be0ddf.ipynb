{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6960,"databundleVersionId":44258,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Bhawesh Pandit","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T09:35:18.489094Z","iopub.execute_input":"2024-11-25T09:35:18.489841Z","iopub.status.idle":"2024-11-25T09:35:18.494511Z","shell.execute_reply.started":"2024-11-25T09:35:18.489806Z","shell.execute_reply":"2024-11-25T09:35:18.49344Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\npaths = []\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        paths.append(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T13:06:07.547067Z","iopub.execute_input":"2024-11-25T13:06:07.547838Z","iopub.status.idle":"2024-11-25T13:06:08.519975Z","shell.execute_reply.started":"2024-11-25T13:06:07.547773Z","shell.execute_reply":"2024-11-25T13:06:08.518876Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# unpack data to working folder \nimport zipfile\nfor file in paths:\n    with zipfile.ZipFile(file, 'r') as zip_ref:\n        zip_ref.extractall('/kaggle/working')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T13:06:08.627785Z","iopub.execute_input":"2024-11-25T13:06:08.628167Z","iopub.status.idle":"2024-11-25T13:06:12.229535Z","shell.execute_reply.started":"2024-11-25T13:06:08.628139Z","shell.execute_reply":"2024-11-25T13:06:12.228491Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import warnings\nimport numpy as np \nimport pandas as pd \nfrom tqdm import tqdm\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n\nfrom haversine import haversine\nfrom sklearn.model_selection import train_test_split\n\ntqdm.pandas()\nwarnings.simplefilter(action='ignore', category=FutureWarning)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T13:06:12.231711Z","iopub.execute_input":"2024-11-25T13:06:12.232485Z","iopub.status.idle":"2024-11-25T13:06:13.749423Z","shell.execute_reply.started":"2024-11-25T13:06:12.232437Z","shell.execute_reply":"2024-11-25T13:06:13.748757Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Splitting: (Max 0 Points)","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/working/train.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T13:06:13.750309Z","iopub.execute_input":"2024-11-25T13:06:13.750658Z","iopub.status.idle":"2024-11-25T13:06:17.688965Z","shell.execute_reply.started":"2024-11-25T13:06:13.750633Z","shell.execute_reply":"2024-11-25T13:06:17.688237Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# EDA on Training Data (Max 3 Points)\n\n1. Identify and Handle Anomalies:\n\n\t- Analyze the daily trip count and identify two anomalous points in the data (e.g., unusually high or low demand).\n\t- Create a new categorical feature `is_anomaly`, where:\n\t- 1 indicates days with abnormal demand.\n\t- 0 indicates regular days.\n\n2. Extract useful temporal features from the `pickup_datetime` column:\n\t- `pickup_day_of_year` (ordinal day of the year)\n\t- `pickup_day_of_week` (0=Monday, 6=Sunday)\n\t- `pickup_hour_of_day` (hour of the day)\n\tThese features will help the model capture time-based patterns.\n\n3. Analyze Traffic Congestion:\n\tCalculate the average speed (distance divided by trip duration) for trips based on the day of the week and hour of the day.\n\tUse this analysis to create a categorical feature for traffic congestion levels, with categories such as:\n\t- 0: Light traffic\n\t- 1: Heavy traffic\n\n4. Passenger Count Analysis:\n\t- Explore the passenger_count column to identify:\n\t- Common values (e.g., 1 or 2 passengers).\n\t- Outliers (e.g., trips with zero or unusually high passenger counts).\n\t- Decide whether to retain or remove these outliers and explain your choice.\n\n5. Using `pickup_longitude`, `pickup_latitude`, `dropoff_longitude`, and `dropoff_latitude`, compute the straight-line distance (trip_distance) using the `haversine` library.\n\n6. Airport Proximity Features: Create binary features to indicate whether the trip started or ended at one of NYC’s major airports:\n\n7. Create binary features indicating whether the trip started or ended within the NYC boundaries. Use bounding box limits or geospatial libraries to define NYC. Use boundaries from previous HW.\n\n8. Analyze the NYC map and improve the distance calculation.\n\n9. Handle Geospatial Outliers: Identify and remove outliers in the test and training datasets based on unrealistic coordinates (e.g., trips starting in the ocean or far outside NYC).\n\n10. Analyze and Clean Trip Duration:\n\t- Plot the distribution of trip_duration and identify outliers (e.g., trips that are excessively short or long).\n\t- Remove these outliers if they significantly distort the data, and justify your decision.\n","metadata":{"execution":{"iopub.status.busy":"2024-11-07T10:13:17.874906Z","iopub.execute_input":"2024-11-07T10:13:17.875723Z","iopub.status.idle":"2024-11-07T10:13:17.89203Z","shell.execute_reply.started":"2024-11-07T10:13:17.875676Z","shell.execute_reply":"2024-11-07T10:13:17.890461Z"}}},{"cell_type":"markdown","source":"\n### 1. Identify and Handle Anomalies:\n\n\t- Analyze the daily trip count and identify two anomalous points in the data (e.g., unusually high or low demand).\n\t- Create a new categorical feature `is_anomaly`, where:\n\t- 1 indicates days with abnormal demand.\n\t- 0 indicates regular days.","metadata":{}},{"cell_type":"code","source":"# Convert pickup_datetime to datetime format and convert to date pickup_date\ndf['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\ndf['pickup_date'] = df['pickup_datetime'].dt.date\n\n# Calculate daily trip counts\ndaily_trips = df.groupby('pickup_date').size().reset_index(name='trip_count')\n\n# Calculate mean and standard deviation\nmean_trips = daily_trips['trip_count'].mean()\nstd_trips = daily_trips['trip_count'].std()\n\n# this is my threshold I checked 1,2,3 and choose 1.5\nthreshold = 1.5\n\nlower_bound = mean_trips - threshold * std_trips\nupper_bound = mean_trips + threshold * std_trips\nprint(lower_bound, upper_bound)\nhigh_anomaly = daily_trips[daily_trips['trip_count'] > upper_bound]\nlow_anomaly = daily_trips[daily_trips['trip_count'] < lower_bound]\n\n# Combine high and low anomalies\nanomalous_days = pd.concat([high_anomaly, low_anomaly])\n\ndf['is_anomaly'] = 0\ndf.loc[df['pickup_date'].isin(anomalous_days['pickup_date']), 'is_anomaly'] = 1\n\n# Visualize the results\nplt.figure(figsize=(12, 6))\nplt.plot(daily_trips['pickup_date'], daily_trips['trip_count'], label='Daily trip cout')\nplt.axhline(y=lower_bound, color='g', linestyle='--', label='Upper Threshold')\nplt.axhline(y=upper_bound,  color='g', linestyle='--', label='Lower Threshold')\nplt.scatter(anomalous_days['pickup_date'], anomalous_days['trip_count'], color='red', label='Anomalies')\nplt.title('1. Identify and Handle Anomalies')\nplt.xlabel('Date')\nplt.ylabel('Daily trip count')\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T13:06:17.690969Z","iopub.execute_input":"2024-11-25T13:06:17.691351Z","iopub.status.idle":"2024-11-25T13:06:19.083227Z","shell.execute_reply.started":"2024-11-25T13:06:17.691315Z","shell.execute_reply":"2024-11-25T13:06:19.08239Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2. Extract useful temporal features from the `pickup_datetime` column:\n\t- `pickup_day_of_year` (ordinal day of the year)\n\t- `pickup_day_of_week` (0=Monday, 6=Sunday)\n\t- `pickup_hour_of_day` (hour of the day)\n\tThese features will help the model capture time-based patterns.\n","metadata":{}},{"cell_type":"code","source":"df['pickup_day_of_year'] = df['pickup_datetime'].dt.year\ndf['pickup_day_of_week'] = df['pickup_datetime'].dt.weekday  # 0=Monday, 6=Sunday\ndf['pickup_hour_of_day'] = df['pickup_datetime'].dt.hour\n\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T13:06:19.084322Z","iopub.execute_input":"2024-11-25T13:06:19.08467Z","iopub.status.idle":"2024-11-25T13:06:19.255523Z","shell.execute_reply.started":"2024-11-25T13:06:19.084634Z","shell.execute_reply":"2024-11-25T13:06:19.254585Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5. Using `pickup_longitude`, `pickup_latitude`, `dropoff_longitude`, and `dropoff_latitude`, compute the straight-line distance (trip_distance) using the `haversine` library.\n","metadata":{}},{"cell_type":"code","source":"def vec_haversine(row):\n    return haversine((row['pickup_latitude'], row['pickup_longitude']),(row['dropoff_latitude'], row['dropoff_longitude']))\n\ndf['trip_distance'] = df.progress_apply(vec_haversine, axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T13:06:19.256427Z","iopub.execute_input":"2024-11-25T13:06:19.25669Z","iopub.status.idle":"2024-11-25T13:06:41.843039Z","shell.execute_reply.started":"2024-11-25T13:06:19.256665Z","shell.execute_reply":"2024-11-25T13:06:41.842169Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3. Analyze Traffic Congestion:\n\tCalculate the average speed (distance divided by trip duration) for trips based on the day of the week and hour of the day.\n\tUse this analysis to create a categorical feature for traffic congestion levels, with categories such as:\n\t- 0: Light traffic\n\t- 1: Heavy traffic","metadata":{}},{"cell_type":"code","source":"# Calculate speed (distance in km / duration in hours)\ndf['speed'] = df['trip_distance'] / (df['trip_duration'] / 3600)\n\n# Calculate average speed by day of the week and hour\ndf_avg = df.groupby(['pickup_day_of_week', 'pickup_hour_of_day'])['speed'].mean().reset_index()\n\n# Visualize to make decision \nfig, ax = plt.subplots(figsize=(10, 10))\ndf_temp = df_avg.pivot(index='pickup_hour_of_day', columns='pickup_day_of_week', values='speed')\nsns.heatmap(df_temp, annot=True, ax=ax)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T13:06:41.844248Z","iopub.execute_input":"2024-11-25T13:06:41.84461Z","iopub.status.idle":"2024-11-25T13:06:42.499519Z","shell.execute_reply.started":"2024-11-25T13:06:41.84457Z","shell.execute_reply":"2024-11-25T13:06:42.498672Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_avg['traffic_congestion'] = 0\n# Based on heatmap I choosed this: \ndf_avg.loc[(((8 <= df_avg['pickup_hour_of_day']) & (df_avg['pickup_hour_of_day'] <= 18)) & ((1 <= df_avg['pickup_day_of_week']) & (df_avg['pickup_day_of_week'] <= 4))), 'traffic_congestion'] = 1\ndf_grb = df_avg.groupby(['pickup_day_of_week', 'pickup_hour_of_day'])[['traffic_congestion']].max().reset_index()\ndf_temp = df_grb.pivot(index='pickup_hour_of_day', columns='pickup_day_of_week', values='traffic_congestion')\n\nfig, ax = plt.subplots(figsize=(10, 10))\nsns.heatmap(df_temp, annot=True, ax=ax)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T13:06:42.502493Z","iopub.execute_input":"2024-11-25T13:06:42.503195Z","iopub.status.idle":"2024-11-25T13:06:43.070966Z","shell.execute_reply.started":"2024-11-25T13:06:42.50315Z","shell.execute_reply":"2024-11-25T13:06:43.070132Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# My trafic condition convert to train dataframe\ndf['traffic_congestion'] = 0\ndf.loc[(((8 <= df['pickup_hour_of_day']) & (df['pickup_hour_of_day'] <= 18)) & ((1 <= df['pickup_day_of_week']) & (df['pickup_day_of_week'] <= 4))), 'traffic_congestion'] = 1\n\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T13:06:43.072238Z","iopub.execute_input":"2024-11-25T13:06:43.072947Z","iopub.status.idle":"2024-11-25T13:06:43.112981Z","shell.execute_reply.started":"2024-11-25T13:06:43.072904Z","shell.execute_reply":"2024-11-25T13:06:43.112255Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4. Passenger Count Analysis:\n\t- Explore the passenger_count column to identify:\n\t- Common values (e.g., 1 or 2 passengers).\n\t- Outliers (e.g., trips with zero or unusually high passenger counts).\n\t- Decide whether to retain or remove these outliers and explain your choice.\n","metadata":{}},{"cell_type":"code","source":"# Check unique values and their counts\npassenger_counts = df['passenger_count'].value_counts().sort_index()\nprint(passenger_counts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T13:06:43.114252Z","iopub.execute_input":"2024-11-25T13:06:43.11492Z","iopub.status.idle":"2024-11-25T13:06:43.128456Z","shell.execute_reply.started":"2024-11-25T13:06:43.114879Z","shell.execute_reply":"2024-11-25T13:06:43.127547Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# I decided to remove 7, 8, 9 based on the counts very small, and I keep 0 because it could be realistic scenario\n# Filter for common values and decide outliers\noutliers = [7, 8, 9]\ndf = df[~df['passenger_count'].isin(outliers)]\n\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T13:06:43.129561Z","iopub.execute_input":"2024-11-25T13:06:43.129868Z","iopub.status.idle":"2024-11-25T13:06:43.349161Z","shell.execute_reply.started":"2024-11-25T13:06:43.129841Z","shell.execute_reply":"2024-11-25T13:06:43.348279Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n### 6. Airport Proximity Features: Create binary features to indicate whether the trip started or ended at one of NYC’s major airports:\n","metadata":{}},{"cell_type":"code","source":"# Define NYC airports coordinates (approximate):\nairports = [\n    (40.6413, -73.7781), # JFK\n    (40.7769, -73.8740), # LaGuardia\n    (40.6895, -74.1745) # Newark\n]\n\ndef is_near_airport(lat, lon, airport_coords, radius=1.5):\n    return haversine((lat, lon), airport_coords) <= radius\n\n# Apply to each row to check if the pickup or dropoff is near an airport\ndf['pickup_near_airport'] = df.progress_apply(lambda x: any(is_near_airport(x['pickup_latitude'], x['pickup_longitude'], airport) for airport in airports), axis=1)\ndf['dropoff_near_airport'] = df.progress_apply(lambda x: any(is_near_airport(x['dropoff_latitude'], x['dropoff_longitude'], airport) for airport in airports), axis=1)\n\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T13:06:43.350332Z","iopub.execute_input":"2024-11-25T13:06:43.350694Z","iopub.status.idle":"2024-11-25T13:07:46.823052Z","shell.execute_reply.started":"2024-11-25T13:06:43.350645Z","shell.execute_reply":"2024-11-25T13:07:46.822164Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n### 7. Create binary features indicating whether the trip started or ended within the NYC boundaries. Use bounding box limits or geospatial libraries to define NYC. Use boundaries from previous HW.\n","metadata":{}},{"cell_type":"code","source":"manhattan_bounds = {\n    'lat_min': 40.70,  \n    'lat_max': 40.88, \n    'lon_min': -74.02, \n    'lon_max': -73.90\n}\n\ndef is_within_nyc(lat, lon):\n    return (manhattan_bounds['lat_min'] <= lat <= manhattan_bounds['lat_max'] and manhattan_bounds['lon_min'] <= lon <= manhattan_bounds['lon_max'])\n\ndf['pickup_in_nyc'] = df.progress_apply(lambda x: is_within_nyc(x['pickup_latitude'], x['pickup_longitude']), axis=1)\ndf['dropoff_in_nyc'] = df.progress_apply(lambda x: is_within_nyc(x['dropoff_latitude'], x['dropoff_longitude']), axis=1)\n\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T13:07:46.824136Z","iopub.execute_input":"2024-11-25T13:07:46.82439Z","iopub.status.idle":"2024-11-25T13:08:15.617364Z","shell.execute_reply.started":"2024-11-25T13:07:46.824365Z","shell.execute_reply":"2024-11-25T13:08:15.616546Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n### 8. Analyze the NYC map and improve the distance calculation.\n","metadata":{}},{"cell_type":"code","source":"!pip install osmnx\nimport osmnx as ox\n\nG = ox.graph_from_place('New York, USA', network_type='drive')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T09:59:55.951928Z","iopub.execute_input":"2024-11-25T09:59:55.952347Z","iopub.status.idle":"2024-11-25T10:01:30.340666Z","shell.execute_reply.started":"2024-11-25T09:59:55.952303Z","shell.execute_reply":"2024-11-25T10:01:30.339836Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\ndef vec_osmnx(row):\n    try:\n        pickup_node = ox.distance.nearest_nodes(G, row['pickup_longitude'], row['pickup_latitude'])\n        dropoff_node = ox.distance.nearest_nodes(G, row['dropoff_longitude'], row['dropoff_latitude'])\n        route_nodes = ox.shortest_path(G, pickup_node, dropoff_node, weight=\"length\", cpus=None)\n        edge_lengths = ox.routing.route_to_gdf(G, route_nodes)\n        return sum(edge_lengths[\"length\"])/1000\n    except:\n        return row['trip_distance']\n\ndf['trip_distance_ox'] = df.progress_apply(vec_osmnx, axis=1)\n\n# It took too long, I decided not to use it.\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T10:01:30.342917Z","iopub.execute_input":"2024-11-25T10:01:30.343421Z","iopub.status.idle":"2024-11-25T10:01:30.349523Z","shell.execute_reply.started":"2024-11-25T10:01:30.343391Z","shell.execute_reply":"2024-11-25T10:01:30.348688Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nrow = df.loc[df['trip_distance'] == df['trip_distance'].max()]\npickup_longitude = row.iloc[0]['pickup_longitude']\npickup_latitude = row.iloc[0]['pickup_latitude']\ndropoff_longitude = row.iloc[0]['dropoff_longitude']\ndropoff_latitude = row.iloc[0]['dropoff_latitude']\n\npickup_node = ox.distance.nearest_nodes(G, pickup_longitude, pickup_latitude)\ndropoff_node = ox.distance.nearest_nodes(G, dropoff_longitude, dropoff_latitude)\nroute_nodes = ox.shortest_path(G, pickup_node, dropoff_node, weight=\"length\", cpus=None)\nedge_lengths = ox.routing.route_to_gdf(G, route_nodes)\n\nprint(row.iloc[0].id, row.iloc[0].trip_distance, sum(edge_lengths[\"length\"]))\nfig, ax = ox.plot_graph_route(G, route_nodes, route_color=\"r\", route_linewidth=6, node_size=0)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T10:01:30.350594Z","iopub.execute_input":"2024-11-25T10:01:30.350943Z","iopub.status.idle":"2024-11-25T10:01:38.862929Z","shell.execute_reply.started":"2024-11-25T10:01:30.350904Z","shell.execute_reply":"2024-11-25T10:01:38.862116Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n### 9. Handle Geospatial Outliers: Identify and remove outliers in the test and training datasets based on unrealistic coordinates (e.g., trips starting in the ocean or far outside NYC).","metadata":{}},{"cell_type":"code","source":"blen = df.shape[0]\ndf = df[df['pickup_in_nyc'] & df['dropoff_in_nyc']]\n\nprint(f'Removed: {blen-df.shape[0]}')\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T13:08:15.61858Z","iopub.execute_input":"2024-11-25T13:08:15.618965Z","iopub.status.idle":"2024-11-25T13:08:15.836093Z","shell.execute_reply.started":"2024-11-25T13:08:15.618928Z","shell.execute_reply":"2024-11-25T13:08:15.835213Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 10. Analyze and Clean Trip Duration:\n\t- Plot the distribution of trip_duration and identify outliers (e.g., trips that are excessively short or long).\n\t- Remove these outliers if they significantly distort the data, and justify your decision.\n","metadata":{}},{"cell_type":"code","source":"df['trip_duration'] = df['trip_duration'].progress_apply(np.log1p)\n\nplt.hist(df['trip_duration'], bins = 100)\nplt.title('log(trip_duration)')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T13:08:15.837236Z","iopub.execute_input":"2024-11-25T13:08:15.837598Z","iopub.status.idle":"2024-11-25T13:08:18.620635Z","shell.execute_reply.started":"2024-11-25T13:08:15.83756Z","shell.execute_reply":"2024-11-25T13:08:18.619813Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_constant = df['trip_duration'].mean()\nprint(best_constant, np.expm1(best_constant))\nmin_duration, max_duration = 4, 8\nprint(np.expm1(min_duration), np.expm1(max_duration))\n'''\nI decided to choose trip duration between 54s and 8102s\n'''\nblen = df.shape[0]\ndf = df[(df['trip_duration'] >= min_duration) & (df['trip_duration'] <= max_duration)]\n\nprint(f'Removed: {blen-df.shape[0]}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T13:08:18.621576Z","iopub.execute_input":"2024-11-25T13:08:18.621845Z","iopub.status.idle":"2024-11-25T13:08:18.825462Z","shell.execute_reply.started":"2024-11-25T13:08:18.621807Z","shell.execute_reply":"2024-11-25T13:08:18.824563Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T13:08:18.826516Z","iopub.execute_input":"2024-11-25T13:08:18.826859Z","iopub.status.idle":"2024-11-25T13:08:19.776913Z","shell.execute_reply.started":"2024-11-25T13:08:18.826823Z","shell.execute_reply":"2024-11-25T13:08:19.776073Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Preprocessing (Max 2 Points)\n\n1. Divide features into numerical and categorical groups and justify your choice:\n\n2. Preprocess Features: \n   \n \tFor numerical features:\n\n \t - Handle missing values (if any).\n \t - Normalize or standardize features to ensure consistent scaling.\n     \n \tFor categorical features:\n\n \t - Encode them using one-hot encoding or ordinal encoding, as appropriate.\n  \n  Use sklearn.pipeline to streamline preprocessing and model training","metadata":{}},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\n\nnumerical_features = [\n    'speed',\n    'trip_distance',\n    'passenger_count',\n    'pickup_hour_of_day',\n]\n\ncategorical_features = [\n    'is_anomaly',\n    'pickup_day_of_week',\n    'traffic_congestion',\n    'pickup_near_airport',\n    'dropoff_near_airport'\n]\n\ndf = df[numerical_features+categorical_features+['trip_duration']]\ntrain, test = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n\nX_train = train.drop(columns=['trip_duration'])\ny_train = train['trip_duration']\nX_test = test.drop(columns=['trip_duration'])\ny_test = test['trip_duration']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T13:08:19.778137Z","iopub.execute_input":"2024-11-25T13:08:19.778858Z","iopub.status.idle":"2024-11-25T13:08:20.173071Z","shell.execute_reply.started":"2024-11-25T13:08:19.778816Z","shell.execute_reply":"2024-11-25T13:08:20.1723Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preprocessor = ColumnTransformer(\n    transformers=[\n        ('scaling', StandardScaler(), numerical_features),\n        ('ohe', OneHotEncoder(handle_unknown=\"ignore\"), categorical_features)\n    ]\n)\n\npipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('model', Ridge())\n])\n\nmodel = pipeline.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\nprint(f\"Train RMSE = {mean_squared_error(y_train, model.predict(X_train), squared=False)}\")\nprint(f\"Test RMSE = {mean_squared_error(y_test, y_pred, squared=False)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T13:08:20.176085Z","iopub.execute_input":"2024-11-25T13:08:20.176355Z","iopub.status.idle":"2024-11-25T13:08:21.965536Z","shell.execute_reply.started":"2024-11-25T13:08:20.17633Z","shell.execute_reply":"2024-11-25T13:08:21.963595Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Training and Evaluation (Max 4 Points)\n\n## Linear model\n1. Use DummyRegressor as a baseline model to predict trip duration.\n2. Use LinearRegression, Ridge, and Lasso Regression from sklearn with Mean Squared Error (MSE) metric to evaluate performance on the test set. For metric evaluation use cross_val_score with `cv=10`\n3. Calculate feathure importance for feathures\n4. Use Lasso regression for feature selection. Is removing feathure improve your result?\n5. Tune Hyperparameters (if needed) using GridSearchCV or RandomizedSearchCV.\n\n## DecisionTreeRegressor\n1. Use DecisionTreeRegressor with Mean Squared Error (MSE) metric to evaluate performance on the test set\n2. Tune Hyperparameters (if needed) using GridSearchCV or RandomizedSearchCV.\n3. Calculate feathure importance for feathures\n\n## RandomForest\n1. Use RandomForestRegressor with Mean Squared Error (MSE) metric to evaluate performance on the test set\n2. Tune Hyperparameters (if needed) using GridSearchCV or RandomizedSearchCV.\n3. Calculate feathure importance for feathures","metadata":{}},{"cell_type":"markdown","source":"## Linear model\n\n### 1. Use DummyRegressor as a baseline model to predict trip duration.\n","metadata":{}},{"cell_type":"code","source":"pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('model', DummyRegressor(strategy=\"mean\"))\n])\ndummy_model = pipeline.fit(X_train, y_train)\ny_pred = dummy_model.predict(X_test)\n\nprint(f\"Dummy Regressor RMSE = {mean_squared_error(y_test, y_pred, squared=False)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T13:08:21.967303Z","iopub.execute_input":"2024-11-25T13:08:21.96776Z","iopub.status.idle":"2024-11-25T13:08:22.826525Z","shell.execute_reply.started":"2024-11-25T13:08:21.967707Z","shell.execute_reply":"2024-11-25T13:08:22.825549Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2. Use LinearRegression, Ridge, and Lasso Regression from sklearn with Mean Squared Error (MSE) metric to evaluate performance on the test set. For metric evaluation use cross_val_score with `cv=10`","metadata":{}},{"cell_type":"code","source":"models = {\n    \"Linear Regression\": LinearRegression(),\n    \"Ridge\": Ridge(),\n    \"Lasso\": Lasso()\n}\n\nfor name, model in models.items():\n    pipeline = Pipeline(steps=[\n        ('preprocessor', preprocessor),\n        ('model', model)\n    ])\n    scores = cross_val_score(pipeline, X_train, y_train, scoring='neg_mean_squared_error', cv=10)\n    mse = -scores.mean()\n    print(f\"{name} MSE: {mse:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T13:50:52.941414Z","iopub.execute_input":"2024-11-25T13:50:52.941759Z","iopub.status.idle":"2024-11-25T13:51:22.952556Z","shell.execute_reply.started":"2024-11-25T13:50:52.941731Z","shell.execute_reply":"2024-11-25T13:51:22.950709Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3. Calculate feature importance for features","metadata":{}},{"cell_type":"code","source":"lr = LinearRegression()\nlr.fit(X_train, y_train)\n\nselected_features = pd.Series(lr.coef_, index=X_train.columns).sort_values(ascending=False)\n\nprint(\"Linear Regression Feature Importance:\")\nprint(selected_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T13:09:00.519638Z","iopub.execute_input":"2024-11-25T13:09:00.51999Z","iopub.status.idle":"2024-11-25T13:09:01.086728Z","shell.execute_reply.started":"2024-11-25T13:09:00.519953Z","shell.execute_reply":"2024-11-25T13:09:01.085701Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (8.0, 8.0)\nselected_features.plot(kind = \"barh\")\nplt.title(\"Feature importance using Linear Regression Model\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T13:09:01.088048Z","iopub.execute_input":"2024-11-25T13:09:01.088369Z","iopub.status.idle":"2024-11-25T13:09:01.348264Z","shell.execute_reply.started":"2024-11-25T13:09:01.088335Z","shell.execute_reply":"2024-11-25T13:09:01.347441Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# I decided  to remove dropoff_near_airport and pickup_near_airport\ncategorical_features = ['is_anomaly', 'pickup_day_of_week', 'traffic_congestion']\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('scaling', StandardScaler(), numerical_features),\n        ('ohe', OneHotEncoder(handle_unknown=\"ignore\"), categorical_features)\n    ]\n)\nfor name, model in models.items():\n    pipeline = Pipeline(steps=[\n        ('preprocessor', preprocessor),\n        ('model', model)\n    ])\n    scores = cross_val_score(pipeline, X_train, y_train, scoring='neg_mean_squared_error', cv=10)\n    mse = -scores.mean()\n    print(f\"{name} with selected features MSE: {mse:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T13:54:18.231934Z","iopub.execute_input":"2024-11-25T13:54:18.232465Z","iopub.status.idle":"2024-11-25T13:54:48.507407Z","shell.execute_reply.started":"2024-11-25T13:54:18.232432Z","shell.execute_reply":"2024-11-25T13:54:48.503842Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4. Use Lasso regression for feature selection. Is removing feathure improve your result?","metadata":{}},{"cell_type":"code","source":"lasso = Lasso(alpha=1e-4)\nlasso.fit(X_train, y_train)\n\nselected_features = pd.Series(lasso.coef_, index=X_train.columns).sort_values(ascending=False)\n\nprint(\"Lasso Feature Importance:\")\nprint(selected_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T13:09:31.086942Z","iopub.execute_input":"2024-11-25T13:09:31.090075Z","iopub.status.idle":"2024-11-25T13:09:31.580065Z","shell.execute_reply.started":"2024-11-25T13:09:31.09002Z","shell.execute_reply":"2024-11-25T13:09:31.579335Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\nLinear and Lasso Regression are I get around same result so I decide to not to use\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T13:09:31.580987Z","iopub.execute_input":"2024-11-25T13:09:31.581284Z","iopub.status.idle":"2024-11-25T13:09:31.589255Z","shell.execute_reply.started":"2024-11-25T13:09:31.581252Z","shell.execute_reply":"2024-11-25T13:09:31.585778Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5. Tune Hyperparameters (if needed) using GridSearchCV or RandomizedSearchCV.","metadata":{}},{"cell_type":"code","source":"param_grid = {'alpha': np.logspace(-2, 3, 10)}\ngrid = GridSearchCV(Ridge(), param_grid, cv=5, scoring='neg_mean_squared_error')\ngrid.fit(X_train, y_train)\n\nprint(f\"Best parameters for Ridge: {grid.best_params_['alpha']:.3f}\")\nprint(f\"Best MSE: {-grid.best_score_:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T13:09:31.59017Z","iopub.execute_input":"2024-11-25T13:09:31.590622Z","iopub.status.idle":"2024-11-25T13:09:44.78587Z","shell.execute_reply.started":"2024-11-25T13:09:31.590588Z","shell.execute_reply":"2024-11-25T13:09:44.784875Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## DecisionTreeRegressor\n### 1. Use DecisionTreeRegressor with Mean Squared Error (MSE) metric to evaluate performance on the test set","metadata":{}},{"cell_type":"code","source":"%%time\nfrom sklearn.tree import DecisionTreeRegressor\n\ndtr = DecisionTreeRegressor(random_state = 42)\ndtr.fit(X_train, y_train)\nmse = mean_squared_error(y_test, dtr.predict(X_test))\n\nprint(f\"Decision Tree Regressor MSE: {mse:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T13:09:44.787185Z","iopub.execute_input":"2024-11-25T13:09:44.788135Z","iopub.status.idle":"2024-11-25T13:09:51.692618Z","shell.execute_reply.started":"2024-11-25T13:09:44.788094Z","shell.execute_reply":"2024-11-25T13:09:51.691766Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2. Tune Hyperparameters (if needed) using GridSearchCV or RandomizedSearchCV.","metadata":{}},{"cell_type":"code","source":"%%time\nparam_grid = {\n    'max_depth': [5, 10],\n    'min_samples_leaf': [2, 4],\n    'max_features': ['auto', 'sqrt', 'log2']\n}\ngrid_dtr = GridSearchCV(DecisionTreeRegressor(random_state=42), param_grid, scoring='neg_mean_squared_error', cv=5)\ngrid_dtr.fit(X_train, y_train)\n\nprint(\"Best parameters for Random Forest:\", grid_dtr.best_params_)\nprint(f\"Best MSE: {-grid_dtr.best_score_:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T13:09:51.694149Z","iopub.execute_input":"2024-11-25T13:09:51.694519Z","iopub.status.idle":"2024-11-25T13:11:18.668854Z","shell.execute_reply.started":"2024-11-25T13:09:51.694482Z","shell.execute_reply":"2024-11-25T13:11:18.668064Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3. Calculate feathure importance for feathures","metadata":{}},{"cell_type":"code","source":"best_dtr = grid_dtr.best_estimator_\nimportances = best_dtr.feature_importances_\n\nselected_features = pd.Series(importances, index=X_train.columns).sort_values(ascending=False)\nprint(\"Feature Importance for Tuned Decision Tree:\")\nprint(selected_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T13:11:18.669938Z","iopub.execute_input":"2024-11-25T13:11:18.670175Z","iopub.status.idle":"2024-11-25T13:11:18.677056Z","shell.execute_reply.started":"2024-11-25T13:11:18.670152Z","shell.execute_reply":"2024-11-25T13:11:18.676013Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## RandomForest\n### 1. Use RandomForestRegressor with Mean Squared Error (MSE) metric to evaluate performance on the test set\n","metadata":{}},{"cell_type":"code","source":"%%time\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor(n_estimators=50, random_state=42, max_features=3)\nrf.fit(X_train, y_train)\n\nmse = mean_squared_error(y_test, rf.predict(X_test))\n\nprint(f\"Random Forest MSE: {mse:.4}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T13:11:18.678111Z","iopub.execute_input":"2024-11-25T13:11:18.678349Z","iopub.status.idle":"2024-11-25T13:13:05.903543Z","shell.execute_reply.started":"2024-11-25T13:11:18.678316Z","shell.execute_reply":"2024-11-25T13:13:05.902536Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2. Tune Hyperparameters (if needed) using GridSearchCV or RandomizedSearchCV.","metadata":{}},{"cell_type":"code","source":"%%time\nparam_grid = {\n    'n_estimators': [50, 100],\n    'max_depth': [5, 10],\n    'min_samples_leaf': [2, 4]\n}\n\ngrid_rf = GridSearchCV(RandomForestRegressor(random_state = 42), param_grid, scoring='neg_mean_squared_error', cv=5)\ngrid_rf.fit(X_train, y_train)\n\nprint(\"Best parameters for Random Forest:\", grid_rf.best_params_)\nprint(f\"Best MSE: {-grid_rf.best_score_:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T13:22:14.944779Z","iopub.execute_input":"2024-11-25T13:22:14.945161Z","iopub.status.idle":"2024-11-25T13:28:41.742435Z","shell.execute_reply.started":"2024-11-25T13:22:14.945132Z","shell.execute_reply":"2024-11-25T13:28:41.741674Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3. Calculate feathure importance for feathures","metadata":{}},{"cell_type":"code","source":"best_rf = grid_rf.best_estimator_\nimportances = best_rf.feature_importances_\n\nselected_features = pd.Series(importances, index=X_train.columns).sort_values(ascending=False)\nprint(\"Feature Importance from Random Forest:\")\nprint(selected_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T13:28:41.744405Z","iopub.execute_input":"2024-11-25T13:28:41.745077Z","iopub.status.idle":"2024-11-25T13:28:41.754656Z","shell.execute_reply.started":"2024-11-25T13:28:41.745028Z","shell.execute_reply":"2024-11-25T13:28:41.753774Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Conclusion (Max 1 Points)\n\nSelect best model, explain results","metadata":{}},{"cell_type":"code","source":"dtr = DecisionTreeRegressor(**grid_dtr.best_params_)\ndtr.fit(X_train, y_train)\nmse = mean_squared_error(y_test, dtr.predict(X_test))\n\nprint(f\"Decision Tree MSE: {mse:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T13:13:05.906085Z","iopub.execute_input":"2024-11-25T13:13:05.906351Z","iopub.status.idle":"2024-11-25T13:13:09.789251Z","shell.execute_reply.started":"2024-11-25T13:13:05.906325Z","shell.execute_reply":"2024-11-25T13:13:09.788262Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('model', best_dtr)\n])\n\nscores = cross_val_score(pipeline, X_train, y_train, scoring='neg_mean_squared_error', cv=10)\nmse = -scores.mean()\ntmse = mean_squared_error(y_test, pipeline['model'].predict(X_test))\n\nprint(f\"Decision Tree MSE with selected features: {mse:.4f}\")\nprint(f\"Decision Tree MSE on Test: {tmse:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T13:57:15.428499Z","iopub.execute_input":"2024-11-25T13:57:15.42929Z","iopub.status.idle":"2024-11-25T13:58:00.562231Z","shell.execute_reply.started":"2024-11-25T13:57:15.429258Z","shell.execute_reply":"2024-11-25T13:58:00.561147Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rf = RandomForestRegressor(**grid_rf.best_params_)\nrf.fit(X_train, y_train)\n\nmse = mean_squared_error(y_test, rf.predict(X_test))\n\nprint(f\"Random Forest MSE: {mse:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T13:28:41.755589Z","iopub.execute_input":"2024-11-25T13:28:41.755887Z","iopub.status.idle":"2024-11-25T13:29:59.237821Z","shell.execute_reply.started":"2024-11-25T13:28:41.755858Z","shell.execute_reply":"2024-11-25T13:29:59.23691Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('model', best_rf)\n])\n\nscores = cross_val_score(pipeline, X_train, y_train, scoring='neg_mean_squared_error', cv=10)\nmse = -scores.mean()\n\ntmse = mean_squared_error(y_test, pipeline['model'].predict(X_test))\nprint(f\"Random Forest MSE with selected features: {mse:.4f}\")\nprint(f\"Random Forest MSE on Test: {tmse:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T13:29:59.239168Z","iopub.execute_input":"2024-11-25T13:29:59.239437Z","iopub.status.idle":"2024-11-25T13:43:40.953405Z","shell.execute_reply.started":"2024-11-25T13:29:59.239412Z","shell.execute_reply":"2024-11-25T13:43:40.952538Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Linear Regression MSE with selected features: 0.094\n#Ridge MSE with selected features: 0.094\n#Lasso MSE with selected features: 0.44\n#Decision Tree MSE: 0.004\n#Random Forest MSE: 0.003","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n**I'll choose the Decision Tree instead of the Random Forest. While the Random Forest offers the best performance, its training time is significantly longer, and it is more prone to overfitting in the long run**\n","metadata":{}}]}